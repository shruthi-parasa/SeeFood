# SeeFood ğŸğŸ“±  
*An accessibility-focused food recognition app built with SwiftUI and CoreML.*  

## ğŸ‘¤ My Contributions  
This project was created collaboratively during a hackathon. I contributed by:  
- Working on Figma Design and the Frontend. 
- Assisting with Swift UI code implementation.  
- Writing documentation for the project.

---

## ğŸ“Œ Overview  
SeeFood is an iOS app that helps visually impaired individuals identify food items.  

- ğŸ“¸ Take a picture of any food item using the camera  
- ğŸ¤– Classify the item with a **CoreML model**  
- ğŸ”Š Hear an **audio description** via Appleâ€™s VoiceOver accessibility feature  

---

## ğŸ› ï¸ Built With  
- [Swift](https://developer.apple.com/swift/) / [SwiftUI](https://developer.apple.com/xcode/swiftui/)  
- [Xcode](https://developer.apple.com/xcode/)  
- [CoreML](https://developer.apple.com/machine-learning/)  
- [Figma](https://www.figma.com/)  

---

## ğŸ‘¥ Credits  
This project was built by a **team of 4** during a 24-hour hackathon (Hack Davis 2023)  

Original project: [Devpost](https://devpost.com/software/seefood-bcs7y0)  
