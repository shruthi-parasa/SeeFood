# SeeFood 🍎📱  
*An accessibility-focused food recognition app built with SwiftUI and CoreML.*  

## 👤 My Contributions  
This project was created collaboratively during a hackathon. I contributed by:  
- Working on Figma Design and the Frontend. 
- Assisting with Swift UI code implementation.  
- Writing documentation for the project.

---

## 📌 Overview  
SeeFood is an iOS app that helps visually impaired individuals identify food items.  

- 📸 Take a picture of any food item using the camera  
- 🤖 Classify the item with a **CoreML model**  
- 🔊 Hear an **audio description** via Apple’s VoiceOver accessibility feature  

---

## 🛠️ Built With  
- [Swift](https://developer.apple.com/swift/) / [SwiftUI](https://developer.apple.com/xcode/swiftui/)  
- [Xcode](https://developer.apple.com/xcode/)  
- [CoreML](https://developer.apple.com/machine-learning/)  
- [Figma](https://www.figma.com/)  

---

## 👥 Credits  
This project was built by a **team of 4** during a 24-hour hackathon (Hack Davis 2023)  

Original project: [Devpost](https://devpost.com/software/seefood-bcs7y0)  
